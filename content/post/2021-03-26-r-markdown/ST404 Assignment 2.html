---
title: "USA Crime Analysis"
author: "Zhicong Hu"
date: 2021-03-26T21:13:14-05:00
categories: ["R"]
tags: ["R Markdown", "plot", "regression"]

header-includes: 
- \usepackage{float}
---

<script src="ST404 Assignment 2_files/kePrint/kePrint.js"></script>
<link href="ST404 Assignment 2_files/lightable/lightable.css" rel="stylesheet" />


<style type="text/css">
body, td {
   font-size: 11px;
}
code.r{
  font-size: 9px;
}
pre {
  font-size: 9px
}
</style>
<div style="page-break-after: always;"></div>
<div id="findings" class="section level1">
<h1>Findings</h1>
<div id="recommendations-from-exploratory-data-analysis" class="section level2">
<h2>Recommendations from Exploratory Data Analysis</h2>
<p>A key part of the model-making process is applying the results of Exploratory Data Analysis to ensure that our data is “clean” (i.e., it contains no faulty or incorrect values) by transforming variables to address their skew/relationship with other variables and potentially exclude variables from the model to make it simpler. The recommended adjustments to the USACrime data are:</p>
<ul>
<li>Remove entries in the data set with missing values (in <strong>medIncome</strong> and <strong>pctEmploy</strong>) as well as entries that appear to be missing values, since we found this data to be missing completely at random. Additionally, leave the outliers in the data set before modelling.</li>
<li>Ignore states that have no data entries and combine the “Pacific” region with the “West” region, this reduces the number of levels for this variable to 4.</li>
<li>Apply several transformations to the outcome and predictor variables. These are listed in the Statistical Methodology.</li>
<li>Omit certain variables which represent methods of encoding similar information: we choose <strong>medIncome</strong> for income/wealth, <strong>pctKidsBornNeverMarr</strong> for family, and <strong>pctLowEdu</strong> for education.</li>
</ul>
</div>
<div id="modelling" class="section level2">
<h2>Modelling</h2>
<p>After making changes to our data, we go through the process of developing our linear regression models. Of the various penalized likelihood strategies, we choose <strong>LASSO regression</strong> as it is both a penalized likelihood strategy and a form of variable selection, which will reduce model complexity.</p>
<p>After going through the process of LASSO regression, we end up with a model for predicting violent crime with the corresponding variables and coefficients. It is important to note that these values, which are from one run of the LASSO regression, may not be the same as the ones calculated in the Statistical Methodology. This has to do with how LASSO finds the optimal <span class="math inline">\(\lambda\)</span>, which can calculate a different value every time the process is run. However, these coefficients are similar enough to a typical LASSO regression to draw conclusions from.</p>
<table class="kable_wrapper table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:unnamed-chunk-3">Table 1: </span>Violent crime and non-violent crime model variables and coefficients
</caption>
<tbody>
<tr>
<td>
<table>
<thead>
<tr>
<th style="text-align:left;">
Variable
</th>
<th style="text-align:left;">
Coefficient
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Intercept
</td>
<td style="text-align:left;">
10.6298
</td>
</tr>
<tr>
<td style="text-align:left;">
pctUrban
</td>
<td style="text-align:left;">
0.1619
</td>
</tr>
<tr>
<td style="text-align:left;">
medIncome
</td>
<td style="text-align:left;">
-0.1697
</td>
</tr>
<tr>
<td style="text-align:left;">
pctLowEdu
</td>
<td style="text-align:left;">
0.0127
</td>
</tr>
<tr>
<td style="text-align:left;">
pctUnemploy
</td>
<td style="text-align:left;">
0.0958
</td>
</tr>
<tr>
<td style="text-align:left;">
pctKidsBornNevrMarr
</td>
<td style="text-align:left;">
0.9568
</td>
</tr>
<tr>
<td style="text-align:left;">
pctHousOccup
</td>
<td style="text-align:left;">
-1.677777e-06
</td>
</tr>
<tr>
<td style="text-align:left;">
pctHousOwnerOccup
</td>
<td style="text-align:left;">
-0.0012
</td>
</tr>
<tr>
<td style="text-align:left;">
pctVacantBoarded
</td>
<td style="text-align:left;">
0.0832
</td>
</tr>
<tr>
<td style="text-align:left;">
pctVacant6up
</td>
<td style="text-align:left;">
-0.0831
</td>
</tr>
<tr>
<td style="text-align:left;">
pctForeignBorn
</td>
<td style="text-align:left;">
0.0096
</td>
</tr>
</tbody>
</table>
</td>
<td>
<table>
<thead>
<tr>
<th style="text-align:left;">
Variable
</th>
<th style="text-align:left;">
Coefficient
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Intercept
</td>
<td style="text-align:left;">
18.1484
</td>
</tr>
<tr>
<td style="text-align:left;">
pctUrban
</td>
<td style="text-align:left;">
0.0612
</td>
</tr>
<tr>
<td style="text-align:left;">
medIncome
</td>
<td style="text-align:left;">
-0.3618
</td>
</tr>
<tr>
<td style="text-align:left;">
pctLowEdu
</td>
<td style="text-align:left;">
·
</td>
</tr>
<tr>
<td style="text-align:left;">
pctUnemploy
</td>
<td style="text-align:left;">
·
</td>
</tr>
<tr>
<td style="text-align:left;">
pctKidsBornNevrMarr
</td>
<td style="text-align:left;">
0.3057
</td>
</tr>
<tr>
<td style="text-align:left;">
pctHousOccup
</td>
<td style="text-align:left;">
-8.189186e-07
</td>
</tr>
<tr>
<td style="text-align:left;">
pctHousOwnerOccup
</td>
<td style="text-align:left;">
-0.0043
</td>
</tr>
<tr>
<td style="text-align:left;">
pctVacantBoarded
</td>
<td style="text-align:left;">
0.0291
</td>
</tr>
<tr>
<td style="text-align:left;">
pctVacant6up
</td>
<td style="text-align:left;">
-0.0672
</td>
</tr>
<tr>
<td style="text-align:left;">
pctForeignBorn
</td>
<td style="text-align:left;">
·
</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>
<p>By using this strategy, the variables <strong>pctEmploy</strong> and <strong>popDensity</strong> have their coefficients reduced to zero and are excluded from the model. Therefore, we can conclude that the employment rate and population density are not important factors in predicting levels of violent crime. Conversely, the major takeaway here is that <strong>pctUrban</strong>, <strong>medIncome</strong>, and <strong>pctKidsBornNevrMarr</strong>, with coefficients of 0.1619, -0.1869, and 0.9500 respectively are the most notable determinants for violent crime in a given county. The coefficient of 0.9500 for <strong>pctKidsBornNevrMarr</strong> suggests that a 10% decrease in the percentage of children born to parents who have never married would reduce violent crime by 9.53%. Similarly, the coefficient for <strong>medIncome</strong> means that a 10% increase in Median Income in a county would reduce violent crime by 1.75%.</p>
<p>Looking at our model for non-violent crime, we notice some interesting results. As for the violent crime model, the LASSO process discards <strong>pctEmploy</strong> and <strong>popDensity</strong>. But unlike for violent crime, the non-violent crime model also discards <strong>pctLowEdu</strong>, <strong>pctUnemploy</strong>, and <strong>pctForeignBorn</strong>. This means the model for non-violent crime is simpler than that for violent crime, with fewer variables included.
Additionally, the major determinants of non-violent crime, namely <strong>medIncome</strong> and <strong>pctKidsBornNevrMarr</strong>, with coefficients -0.3618 and 0.3057 respectively, are also two of the most important variables used to predict violent crime.
From this, we gather that while many of the variables associated with higher violent and non-violent crime are the same, the major difference is that <strong>pctLowEdu</strong>, <strong>pctUnemploy</strong>, and <strong>pctForeignBorn</strong> are deemed to be significantly stronger determinants of violent crime than non-violent crime.</p>
<p>We also note that while <strong>pctKidsBornNevrMarr</strong> is again a significant predictor for non-violent crime, it is less important than the violent crime model, with a 10% decrease in the percentage of children born to parents who have never married reducing non-violent crime by 3.17%, which is much less than the associated decrease of 9.53% for violent crime.
However, the effect of an increase in Median Income results in a larger decrease in non-violent crime compared to violent crime. A 10% increase in Median Income results in a 3.4% decrease in non-violent crime, which is approximately double that of the associated decrease for violent crime.
It should be noted that being an urban area has a more significant effect on levels of violent crime than non-violent crime. An area being classified as Urban increases violent crime by 11.7%, compared to non-violent crime which increases by just 4.2%.
These are important differences between causes of the two types of crime for policymakers to consider.</p>
</div>
<div id="model-diagnostics" class="section level2">
<h2>Model Diagnostics</h2>
<p>In model diagnostics, we will check the assumptions of normality of the errors in linear regression. We will plot a Q-Q plot for the residuals of both the models.</p>
<p><img src="/post/2021-03-26-r-markdown/ST404%20Assignment%202_files/figure-html/unnamed-chunk-4-1.png" width="40%" style="display: block; margin: auto;" /><img src="/post/2021-03-26-r-markdown/ST404%20Assignment%202_files/figure-html/unnamed-chunk-4-2.png" width="40%" style="display: block; margin: auto;" /></p>
<p>From the Q-Q plots, we can deduce that:</p>
<ul>
<li><p>Residuals of the model demonstrate normality, this means that there is a linear relationship between our predictor and outcome variables and linear regression is a valid model to use.</p></li>
<li><p>The extreme values for both models are not following the same linear relationship as the rest of the data, this means that there are some communities/states/regions where our model has trouble predicting. We will further discuss this in the limitations of our model.</p></li>
<li><p>Between the Q-Q plot of the two models, the line in our violent model is a lot steeper than the line in our non-violent model. This means that the residuals in our non-violent model are smaller than the ones in our violent model, this corresponds to the lower Mean-Squared Error in our non-violent model compared with the violent model.</p></li>
</ul>
</div>
<div id="residual-analysis" class="section level2">
<h2>Residual Analysis</h2>
<p>Once the model has been created, we can examine how well our model fits the data by creating residual plots. The main findings from doing this are:</p>
<ul>
<li>The shape of the plot (i.e. the distribution) demonstrate that both of our models are generally a good fit for the data.</li>
<li>The non-violent crime model has better predictive power than the violent crime model. We can gather this from the shape of both the Q-Q and residual plots. This is also seen by calculating the Mean-Squared Error, where the lower value for the non-violent crime model also supports these findings.</li>
<li>Generally speaking, it appears the NorthEast region of the US contains the most counties that have abnormally high and low levels of crime; the South also has some areas with unusually high crime rates. We will look at these in more detail.</li>
<li>The violent crime model has more areas with abnormally high/low levels compared to the non-violent crime model, these tend to be in the Northeast, Midwest, and South.</li>
</ul>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-5"></span>
<img src="/post/2021-03-26-r-markdown/ST404%20Assignment%202_files/figure-html/unnamed-chunk-5-1.png" alt="Residual plot for Non-Violent Crime and Violent Crime model" width="60%" />
<p class="caption">
Figure 1: Residual plot for Non-Violent Crime and Violent Crime model
</p>
</div>
</div>
<div id="limitations-of-our-model" class="section level2">
<h2>Limitations of our Model</h2>
<p>Though we apply a number of techniques to create as robust a model as possible, various trade-offs between effectiveness and simplicity should be noted. These include:</p>
<ul>
<li>For the various transformations applied to the data, we often choose ones easier to explain to non-statisticians or ones that may be sub-optimal but not needlessly complex. This could impact larger data values and cause the model to lose some accuracy.</li>
<li>LASSO, the method we use to select variables, has some drawbacks. For example, the number of variables it selects is limited by the number of data observations, which could cause problems with smaller data sets. This may make our models less reproducible.</li>
<li>While we did perform analysis to remove as many variables as possible which displayed evidence of collinearity, we cannot guarantee that the variables removed from our model did not impact the crime rates, as well as this, there could be causation instead of correlation.</li>
<li>Another drawback of LASSO comes from the penalty function. As LASSO adds a penalty function to the model, our model will have a higher Mean-Squared Error compared to a genuine linear model. However, this is an example of a bias-variance trade-off. By increasing the bias, we can prevent overfitting of our model to the training data. This makes sure that our model can work as effectively when predicting future data, which may not be the case without the presence of a penalty function.</li>
<li>There are regions and states where our non-violent model performs poorly. These are regions that have unusually high or low crime rates which do not conform to the general pattern. From the figure below, we can see that the majority of the communities that the model did not work well for come from the NorthEast region. Looking deeper into individual states, MA which stands for Massachusetts stood out in this regard. Therefore, we need to be more aware when using the non-violent model for prediction purposes in the NorthEast region, especially Massachusetts.</li>
</ul>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:residual-plot"></span>
<img src="/post/2021-03-26-r-markdown/ST404%20Assignment%202_files/figure-html/residual-plot-1.png" alt="Residual plot of residuals greater than 2 for our Non-Violent Crime model" width="65%" />
<p class="caption">
Figure 2: Residual plot of residuals greater than 2 for our Non-Violent Crime model
</p>
</div>
<div style="page-break-after: always;"></div>
</div>
</div>
<div id="statistical-methodology" class="section level1">
<h1>Statistical Methodology</h1>
<div id="data-cleaning" class="section level2">
<h2>Data cleaning</h2>
<p>Before creating and iterating our final models, we must take the preliminary step of applying the results of the EDA to clean the USACrime data.</p>
<p>Firstly, we omit the observations that contain the 52 and 21 missing values of <strong>medIncome</strong> and <strong>pctEmploy</strong> respectively, as these have been determined to be MCAR (Missing Completely at Random) during the EDA. Furthermore, there are some entries in the variables <strong>ownHousMed</strong>, <strong>rentMed</strong>, <strong>ownHousQrange</strong>, and <strong>rentQrange</strong> which appear to be entered in place of a missing value. We omit these also, leading to a total of 92 data points being removed. In addition, the results of the EDA suggest that the outliers within the data can be left in before we fit the models.</p>
<p>Additionally, the variable <strong>State</strong> has some empty levels and these are dropped at the beginning to clean up the variable. The variable <strong>region</strong> has a level <strong>Pacific</strong> which only contains three observations, which are all from Alaska. To fix this, the <strong>Pacific</strong> level was merged with <strong>West</strong> as this made the most sense geographically. In the end, <strong>region</strong> has four levels: Midwest, Northeast, West and South.</p>
<p>Finally, from the EDA, several variable transformations were recommended to address the skew of both the outcome and predictor variables. These will improve the robustness of the model, with the predictors becoming much more symmetrical after being transformed. They are:</p>
<ul>
<li>Log2 transformation: <strong>violentPerPop</strong>, <strong>nonViolPerPop</strong> (outcome variables), <strong>medIncome</strong>, <strong>pctLowEdu</strong>, <strong>pctUnemploy</strong></li>
<li>Log2+1 transformation: <strong>pctKidsBornNevrMarr</strong>, <strong>pctVacantBoarded</strong></li>
<li>Power transformation: <strong>pctEmploy</strong> (^2), <strong>pctHousOccup</strong> (^3)</li>
<li>Root transformation: <strong>pctVacant6up</strong>, <strong>popDensity</strong></li>
<li>Categorical transformation: Since <strong>pctUrban</strong> mostly has values which either 0 or 100, <strong>pctUrban</strong> becomes a categorical variable assigning 1 to entries with pctUrban&gt;85 and 0 otherwise</li>
</ul>
<p>Note that we have chosen to use log base 2 as it is easier to interpret for a non-statistician.</p>
</div>
<div id="variable-choice" class="section level2">
<h2>Variable choice</h2>
<p>The first variables we will leave out of the model are the <strong>State</strong> and <strong>region</strong> variables. We suggest dropping the <strong>State</strong> variable, as it has a very large number of levels and may be difficult to interpret. There are also several states with a small number of observations, so the estimates may not be very reliable. For the <strong>region</strong> variable, we determine that regional differences in our data are already captured in the other predictor variables; that is, it would be better to fit the model without <strong>region</strong> and then sort the fitted values by <strong>region</strong> to see how well the model works by region.</p>
<p>In the EDA stage, we found that the variables <strong>medIncome</strong>, <strong>ownHousMed</strong>, <strong>ownHousQrange</strong>, <strong>rentMed</strong> and <strong>rentQrange</strong>, which are related to measuring income/wealth, exhibit a strong positive linear correlation with each other (correlation coefficients above 0.6). To further assess the problem of multicollinearity, we can calculate the Variance Inflation Factor (VIF) values for each variable to see which variables are causing the issue. VIF values over 5-10 indicate the given variable may be highly correlated to at least one of the other variables.</p>
<p>To see which variables are correlated with each other, we then compute the associated variance proportions. There appears to be certain groups of variables that are highly correlated:</p>
<ul>
<li>Income: <strong>medIncome</strong>, <strong>ownHousMed</strong>, <strong>ownHousQrange</strong>, <strong>rentMed</strong> and <strong>rentQrange</strong>, <strong>pctWdiv</strong></li>
<li>Family: <strong>pctKidsNevrMarr</strong>, <strong>pctKids2Par</strong></li>
<li>Education: <strong>pctLowEdu</strong>, <strong>pctNotHSgrad</strong>, <strong>pctCollGrad</strong></li>
</ul>
<p>Upon further examination of these variables, it becomes clear that we only need to include one variable from each category in the model. This helps the model become more parsimonious and reduces the problem of multicollinearity. This is logical, as these variables encode similar information about each county. To decide which variable to include from each group, we have considered which variable is most highly correlated with the outcome variables. Hence we have kept <strong>medIncome</strong>, <strong>pctKidsBornNevrMarr</strong> and <strong>pctLowEdu</strong> in the model and removed the others.</p>
<p>Bear in mind that VIF is not a method of variable selection, with our penalized likelihood strategy being more appropriate. We are merely using VIF to diagnose multicollinearity in our model and gather appropriate conclusions. After removing the variables described above, we can see <strong>medIncome</strong> is the only variable with a VIF between 5 and 10 from this process, with all other variables below 5, which suggests that the issue of multicollinearity has been improved. This means that the variables selected by LASSO and their associated coefficients will be much more robust and can be accurately interpreted for their effect on crime.</p>
<table class="kable_wrapper">
<caption>
<span id="tab:unnamed-chunk-7">Table 2: </span>VIF before and after variable selection
</caption>
<tbody>
<tr>
<td>
<table>
<thead>
<tr>
<th style="text-align:left;">
Variable
</th>
<th style="text-align:left;">
Coefficient
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
pctUrban
</td>
<td style="text-align:left;">
1.5956
</td>
</tr>
<tr>
<td style="text-align:left;">
medIncome
</td>
<td style="text-align:left;">
16.6049
</td>
</tr>
<tr>
<td style="text-align:left;">
pctWdiv
</td>
<td style="text-align:left;">
7.2124
</td>
</tr>
<tr>
<td style="text-align:left;">
pctLowEdu
</td>
<td style="text-align:left;">
8.2135
</td>
</tr>
<tr>
<td style="text-align:left;">
pctNotHSgrad
</td>
<td style="text-align:left;">
11.9291
</td>
</tr>
<tr>
<td style="text-align:left;">
pctCollGrad
</td>
<td style="text-align:left;">
4.5886
</td>
</tr>
<tr>
<td style="text-align:left;">
pctUnemploy
</td>
<td style="text-align:left;">
4.3961
</td>
</tr>
<tr>
<td style="text-align:left;">
pctEmploy
</td>
<td style="text-align:left;">
5.0872
</td>
</tr>
<tr>
<td style="text-align:left;">
pctKids2Par
</td>
<td style="text-align:left;">
10.0282
</td>
</tr>
<tr>
<td style="text-align:left;">
pctKidsBornNevrMarr
</td>
<td style="text-align:left;">
7.1099
</td>
</tr>
<tr>
<td style="text-align:left;">
pctHousOccup
</td>
<td style="text-align:left;">
1.6566
</td>
</tr>
<tr>
<td style="text-align:left;">
pctHousOwnerOccup
</td>
<td style="text-align:left;">
4.379
</td>
</tr>
<tr>
<td style="text-align:left;">
pctVacantBoarded
</td>
<td style="text-align:left;">
1.9866
</td>
</tr>
<tr>
<td style="text-align:left;">
pctVacant6up
</td>
<td style="text-align:left;">
1.8895
</td>
</tr>
<tr>
<td style="text-align:left;">
ownHousMed
</td>
<td style="text-align:left;">
12.6412
</td>
</tr>
<tr>
<td style="text-align:left;">
ownHousQrange
</td>
<td style="text-align:left;">
6.3884
</td>
</tr>
<tr>
<td style="text-align:left;">
rentMed
</td>
<td style="text-align:left;">
9.7279
</td>
</tr>
<tr>
<td style="text-align:left;">
rentQrange
</td>
<td style="text-align:left;">
2.8701
</td>
</tr>
<tr>
<td style="text-align:left;">
popDensity
</td>
<td style="text-align:left;">
2.5185
</td>
</tr>
<tr>
<td style="text-align:left;">
pctForeignBorn
</td>
<td style="text-align:left;">
3.7914
</td>
</tr>
</tbody>
</table>
</td>
<td>
<table>
<thead>
<tr>
<th style="text-align:left;">
Variable
</th>
<th style="text-align:right;">
Coefficient
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
pctUrban
</td>
<td style="text-align:right;">
1.5042
</td>
</tr>
<tr>
<td style="text-align:left;">
medIncome
</td>
<td style="text-align:right;">
6.3634
</td>
</tr>
<tr>
<td style="text-align:left;">
pctLowEdu
</td>
<td style="text-align:right;">
2.8152
</td>
</tr>
<tr>
<td style="text-align:left;">
pctUnemploy
</td>
<td style="text-align:right;">
3.3160
</td>
</tr>
<tr>
<td style="text-align:left;">
pctEmploy
</td>
<td style="text-align:right;">
2.8375
</td>
</tr>
<tr>
<td style="text-align:left;">
pctKidsBornNevrMarr
</td>
<td style="text-align:right;">
2.9202
</td>
</tr>
<tr>
<td style="text-align:left;">
pctHousOccup
</td>
<td style="text-align:right;">
1.3652
</td>
</tr>
<tr>
<td style="text-align:left;">
pctHousOwnerOccup
</td>
<td style="text-align:right;">
3.2498
</td>
</tr>
<tr>
<td style="text-align:left;">
pctVacantBoarded
</td>
<td style="text-align:right;">
1.8900
</td>
</tr>
<tr>
<td style="text-align:left;">
pctVacant6up
</td>
<td style="text-align:right;">
1.7220
</td>
</tr>
<tr>
<td style="text-align:left;">
popDensity
</td>
<td style="text-align:right;">
2.3233
</td>
</tr>
<tr>
<td style="text-align:left;">
pctForeignBorn
</td>
<td style="text-align:right;">
2.5826
</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>
</div>
<div id="modelling-1" class="section level2">
<h2>Modelling</h2>
<p>With the problem of collinearity solved, we go on to building our linear regression models.
We have chosen to use <strong>LASSO regression</strong> for model building for several reasons:</p>
<ol style="list-style-type: decimal">
<li><p>LASSO regression is able to perform <strong>variable selection</strong>, meaning that only variables which help predict our outcome variables will be used in our model and the rest will be removed from the model (coefficient equals zero). This helps to reduce the number of predictors needed in the model, thereby making it more parsimonious and allowing stronger explanatory power.</p></li>
<li><p>LASSO regression uses a <strong>penalized likelihood strategy</strong>. This means that our model is not overfitted to our training data, allowing it to perform strongly when analysing future data. However, this will also result in our model having higher MSE for our training data compared to a normal linear model, but we believe that it is a worthy trade-off between bias and variance in order to improve the performance of the model with future data.</p></li>
</ol>
<p>When considering other approaches to model building, such as Ridge regression or stepwise regression with AIC/BIC we find that they lack some of the benefits LASSO can provide. For example, Ridge regression does not perform variable selection, so does not help to reduce the complexity of the model. Meanwhile, stepwise regression is often guilty of overfitting the model to the training data and as such may not fit future data as well as a model generated using LASSO. Hence we have concluded we will use LASSO regression to build our model.</p>
<div id="lasso-regression-model-for-violent-crimes" class="section level3">
<h3>LASSO regression model for violent Crimes</h3>
<p>While building the LASSO regression model, we used 10-fold Cross-Validation to find the optimal <span class="math inline">\(\lambda\)</span>. The graph below shows <span class="math inline">\(log(\lambda)\)</span> against the Mean-Squared Error of the model with the labels on top showing the number of variables in the model. To maintain our model’s explanatory power, we will restrict the number of variables in our model to a maximum of 10.</p>
<p><img src="/post/2021-03-26-r-markdown/ST404%20Assignment%202_files/figure-html/unnamed-chunk-8-1.png" width="50%" style="display: block; margin: auto;" /></p>
<p>From our graph, we can see that the optimal min value of <span class="math inline">\(\lambda\)</span> is <code>0.0108961</code>, which is shown by the first vertical line. The second vertical line shows the <span class="math inline">\(\lambda\)</span> value with 1 standard error. This is equal to <code>0.0843648</code> and is the value of <span class="math inline">\(\lambda\)</span> that we will be using.</p>
<p>With this value of <span class="math inline">\(\lambda\)</span>, we have our LASSO regression model. The coefficients for each of the variables are:</p>
<table class="table table" style="margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
Variable
</th>
<th style="text-align:left;">
Coefficient
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Intercept
</td>
<td style="text-align:left;">
10.629771
</td>
</tr>
<tr>
<td style="text-align:left;">
pctUrban
</td>
<td style="text-align:left;">
0.161918
</td>
</tr>
<tr>
<td style="text-align:left;">
medIncome
</td>
<td style="text-align:left;">
-0.169687
</td>
</tr>
<tr>
<td style="text-align:left;">
pctLowEdu
</td>
<td style="text-align:left;">
0.012729
</td>
</tr>
<tr>
<td style="text-align:left;">
pctUnemploy
</td>
<td style="text-align:left;">
0.09584
</td>
</tr>
<tr>
<td style="text-align:left;">
pctEmploy
</td>
<td style="text-align:left;">
·
</td>
</tr>
<tr>
<td style="text-align:left;">
pctKidsBornNevrMarr
</td>
<td style="text-align:left;">
0.956776
</td>
</tr>
<tr>
<td style="text-align:left;">
pctHousOccup
</td>
<td style="text-align:left;">
-2e-06
</td>
</tr>
<tr>
<td style="text-align:left;">
pctHousOwnerOccup
</td>
<td style="text-align:left;">
-0.001203
</td>
</tr>
<tr>
<td style="text-align:left;">
pctVacantBoarded
</td>
<td style="text-align:left;">
0.083182
</td>
</tr>
<tr>
<td style="text-align:left;">
pctVacant6up
</td>
<td style="text-align:left;">
-0.083068
</td>
</tr>
<tr>
<td style="text-align:left;">
popDensity
</td>
<td style="text-align:left;">
·
</td>
</tr>
<tr>
<td style="text-align:left;">
pctForeignBorn
</td>
<td style="text-align:left;">
0.009602
</td>
</tr>
</tbody>
</table>
<p>By far the largest coefficient here is <strong>pctKidsBornNevrMarr</strong>, which suggests it is the most important determinant of violent crime, even once the transformations of the variables are taken into account. <strong>medIncome</strong> and <strong>pctUrban</strong> also stand out as having a strong influence on violent crime. The variables <strong>pctEmploy</strong> and <strong>popDensity</strong> are omitted which implies that they have little influence on violent crime rates. The Mean-Squared Error for our model is <code>0.9811852</code>.</p>
</div>
<div id="lasso-regression-model-for-non-violent-crimes" class="section level3">
<h3>LASSO regression model for non-violent crimes</h3>
<p>In a similar fashion to the LASSO model used for predicting violent crime, we also used 10-fold Cross-Validation to find the optimal <span class="math inline">\(\lambda\)</span> for our non-violent crime model. The graph below shows <span class="math inline">\(log(\lambda)\)</span> versus Mean-Squared Error of the non-violent crime model with the top labels showing the number of variables in the model.</p>
<p><img src="/post/2021-03-26-r-markdown/ST404%20Assignment%202_files/figure-html/unnamed-chunk-10-1.png" width="50%" style="display: block; margin: auto;" /></p>
<p>As before, we can use this graph to find our optimal value of <span class="math inline">\(\lambda\)</span>. We will again use the value of <span class="math inline">\(\lambda\)</span> with 1 standard error, as this allows us to make the model more parsimonious while still retaining accuracy in the model. This value of <span class="math inline">\(\lambda\)</span> is given by <code>0.0597405</code>.</p>
<p>With this value of <span class="math inline">\(\lambda\)</span>, we have our LASSO regression model. The coefficients for each of the variables are:</p>
<table class="table table" style="margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
Variable
</th>
<th style="text-align:left;">
Coefficient
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Intercept
</td>
<td style="text-align:left;">
17.902325
</td>
</tr>
<tr>
<td style="text-align:left;">
pctUrban
</td>
<td style="text-align:left;">
0.050111
</td>
</tr>
<tr>
<td style="text-align:left;">
medIncome
</td>
<td style="text-align:left;">
-0.349766
</td>
</tr>
<tr>
<td style="text-align:left;">
pctLowEdu
</td>
<td style="text-align:left;">
·
</td>
</tr>
<tr>
<td style="text-align:left;">
pctUnemploy
</td>
<td style="text-align:left;">
·
</td>
</tr>
<tr>
<td style="text-align:left;">
pctEmploy
</td>
<td style="text-align:left;">
·
</td>
</tr>
<tr>
<td style="text-align:left;">
pctKidsBornNevrMarr
</td>
<td style="text-align:left;">
0.308601
</td>
</tr>
<tr>
<td style="text-align:left;">
pctHousOccup
</td>
<td style="text-align:left;">
-1e-06
</td>
</tr>
<tr>
<td style="text-align:left;">
pctHousOwnerOccup
</td>
<td style="text-align:left;">
-0.004397
</td>
</tr>
<tr>
<td style="text-align:left;">
pctVacantBoarded
</td>
<td style="text-align:left;">
0.021731
</td>
</tr>
<tr>
<td style="text-align:left;">
pctVacant6up
</td>
<td style="text-align:left;">
-0.059321
</td>
</tr>
<tr>
<td style="text-align:left;">
popDensity
</td>
<td style="text-align:left;">
·
</td>
</tr>
<tr>
<td style="text-align:left;">
pctForeignBorn
</td>
<td style="text-align:left;">
·
</td>
</tr>
</tbody>
</table>
<p>The variables with the highest coefficients in the non-violent crime model are <strong>medIncome</strong> and <strong>pctKidsBornNevrMarr</strong>, though the coefficient for the latter variable shows that <strong>pctKidsBornNevrMarr</strong> has a relatively lower impact on non-violent crime compared to violent crime. Additionally, the LASSO process removes the <strong>pctLowEdu</strong>, <strong>pctUnemploy</strong>, and <strong>pctForeignBorn</strong> variables in addition to <strong>pctEmploy</strong> and <strong>popDensity</strong>. We can conclude that these are not crucial factors in predicting non-violent crime; <strong>pctLowEdu</strong>, <strong>pctUnemploy</strong>, and <strong>pctForeignBorn</strong> appear to be much more important for violent crime compared to non-violent crime.</p>
<p>The Mean-Squared Error for our model is <code>0.3586807</code>, which is significantly lower than the Mean-Squared Error in the violent crime model. This suggests that our model for non-violent crime is stronger than our model for violent crime.</p>
</div>
</div>
<div id="residual-analysis-1" class="section level2">
<h2>Residual Analysis</h2>
<p><img src="/post/2021-03-26-r-markdown/ST404%20Assignment%202_files/figure-html/figures-side-1.png" width="50%" /><img src="/post/2021-03-26-r-markdown/ST404%20Assignment%202_files/figure-html/figures-side-2.png" width="50%" /></p>
<p>The above plots show the residuals plotted against the fitted values for the two models. We can see that the assumption of linearity in both our models hold, as shown by the fact that the residuals have approximately mean 0 for all fitted values in both plots.</p>
<p>In the case of the violent crime model, we can see some minor heteroscedasticity in the residual plot, with the residuals appearing more spread out on the left of the plot, with lower fitted values. There are, however, more observations for the lower fitted values and so this may help to explain this phenomenon. The majority of the residuals do appear homoscedastic, which means that our model is still reliable.</p>
<p>For the non-violent model, the residuals are clearly homoscedastic, with the plot exhibiting points that are evenly spread out above and below the x-axis.
This demonstrates the robustness of our model and means that the coefficients for our model are accurate and can be interpreted. In these plots, we can also see the presence of some outliers, which will be talked about in a later section.</p>
<p>In addition to calculating the Mean-Squared Error, we can analyse the residuals for each of our models. We will do this by using Q-Q plots, beginning with the violent crime model.</p>
<p><img src="/post/2021-03-26-r-markdown/ST404%20Assignment%202_files/figure-html/unnamed-chunk-12-1.png" width="50%" /><img src="/post/2021-03-26-r-markdown/ST404%20Assignment%202_files/figure-html/unnamed-chunk-12-2.png" width="50%" /></p>
<p>There are some things that we picked up from the Q-Q plot of the models’ residuals:</p>
<ul>
<li><p><strong>The LASSO model for both violent and non-violent crime is generally a good fit.</strong> From the Q-Q plot, we can deduce that the residuals of the model are normally distributed, this means that there is a linear relationship between our explanatory variables and our outcome variables, violent/non-violent crimes per 100k population.</p></li>
<li><p>Points at the extreme values are not following the same linear relationship as the rest of the data. These points represent extreme values of residuals; these are also points that we investigated earlier where we identify places that have an unusual amount of violent crime.</p></li>
<li><p>However, there is one key difference between our violent and non-violent model. The line in our violent model is a lot steeper than the line in our non-violent model, which means that the residuals in our non-violent model are smaller than the ones in our violent model.</p></li>
</ul>
<p>We can also conduct some investigation of our model to look for outliers. As seen on the residual plots for the non-violent crime model, there appears to be two points with significantly larger residuals than the other observations. To assess whether these points can be considered as outliers, we can look at the Cook’s distance of each point in the plot below. For the non-violent model, the values of the Cook’s distance for the observations with index 320 and 363 are over three times that of any other observation and so can be considered outliers in the model.</p>
<p>When considering the violent crime model, we can see that although there are some values with larger Cook’s distance than the rest of the points, these are nowhere near as extreme as the values seen in the non-violent crime model. It is important to remember that with a large number of observations, it is likely that some will have larger residual values, which does not necessarily make them outliers. Hence we do not consider any of the observations from the violent crime model to be outliers.</p>
<p>To deal with the outliers described in the non-violent crime model, we can fit the model with and without these two points, to see how sensitive the model is to them. After removing these two points and refitting the model, the values of the coefficients hardly change. This is logical, as although these points are influential, they are just 2 of 1810 observations in the entire model. Hence we can proceed to fit the model with these two outliers still included.</p>
<p><img src="/post/2021-03-26-r-markdown/ST404%20Assignment%202_files/figure-html/Cooks%20Distance-1.png" width="60%" style="display: block; margin: auto;" /></p>
</div>
<div id="limitations-of-our-model-1" class="section level2">
<h2>Limitations of our Model</h2>
<p>One of the first limitations of our model is in the transformations applied to the predictor variables. We are in a trade-off between accuracy and interpretability. The more intuitive transformations used do make the model easier to interpret, however, it does mean that we could lose some accuracy in our model, with the model potentially being less suited to the data compared with some more complex transformations. This was also the case if a fractional power was suggested, for most of the transformations, we rounded the suggested power transform, which will impact the larger values of the data. This may result in the densities of some of the variables not being totally symmetrical, as may be possible with a specific power, but it does have the benefit of being easier to interpret and implement for policymakers.</p>
<p>Additionally, at the start, we excluded some variables based on findings from the EDA. While we showed that this helped with the multicollinearity of the variables, we cannot guarantee doing this did not adversely affect the model.</p>
<p>As well, we used the LASSO method for variable selection. One limitation of this method is that if it has to choose between two correlated variables, it makes the choice arbitrarily. Also, the number of variables chosen in this method is limited by the number of data observations. While this did not impact our model, if someone was to recreate our model with a smaller data set, they would get a limited model. This could impact the reproducibility of our model.</p>
<div style="page-break-after: always;"></div>
</div>
</div>
<div id="appendix" class="section level1">
<h1>Appendix</h1>
<pre class="r"><code>#Loads in data, adjust this for own use
load(&quot;USACrime.rda&quot;)

#Loads appropriate packages, which will be useful when creating plots.
library(knitr)
library(car)
library(dplyr)
library(glmnet)
library(ggplot2)

#Data cleaning
USACrime$ownHousMed[USACrime$ownHousMed==500001]&lt;-NA 
USACrime$rentMed[USACrime$rentMed==1001]&lt;-NA 
USACrime$ownHousQrange[USACrime$ownHousQrange==0]&lt;-NA 
USACrime$rentQrange[USACrime$rentQrange==0]&lt;-NA 
USACrime &lt;- na.omit(USACrime)

#Create a copy of the original data for future use
USACrimeCopy &lt;- USACrime

#Fixing State/region variable
USACrime$State&lt;-droplevels(USACrime$State)
levels(USACrime$region) &lt;- c(&quot;Midwest&quot;, &quot;NorthEast&quot;, &quot;West&quot;, &quot;South&quot;, &quot;West&quot;) 

#Removing State and region variables for investigation
USACrime = subset(USACrime, select = -c(State,region))

#Transformation of the outcome variables
USACrime$violentPerPop &lt;- log(USACrime$violentPerPop,2) 
USACrime$nonViolPerPop &lt;- log(USACrime$nonViolPerPop,2)

#Categorical Transformation
USACrime = USACrime %&gt;% mutate(pctUrban = ifelse(pctUrban &gt;= 85,1,0))

#Log2 Transformations
USACrime$medIncome &lt;- log(USACrime$medIncome,2)
USACrime$pctLowEdu &lt;- log(USACrime$pctLowEdu,2)
USACrime$pctUnemploy &lt;- log(USACrime$pctUnemploy,2)

#Log2+1 Transformations
USACrime$pctKidsBornNevrMarr &lt;- log(USACrime$pctKidsBornNevrMarr+1,2)
USACrime$pctVacantBoarded &lt;- log(USACrime$pctVacantBoarded+1,2)

#Power Transformations
USACrime$pctEmploy &lt;- (USACrime$pctEmploy)^2
USACrime$pctHousOccup &lt;- (USACrime$pctHousOccup)^3

#Root Transformations
USACrime$pctVacant6up &lt;- sqrt(USACrime$pctVacant6up)
USACrime$popDensity &lt;- sqrt(USACrime$popDensity)

#Calculating VIF
model1 &lt;- lm(violentPerPop ~ pctUrban + medIncome + pctLowEdu +
               pctUnemploy + pctEmploy 
+ pctKidsBornNevrMarr + pctHousOccup +
               pctHousOwnerOccup + pctVacantBoarded + 
pctVacant6up + popDensity + pctForeignBorn, data = USACrime)
vif(model1)

model1 &lt;- lm(violentPerPop ~ pctUrban + medIncome + pctLowEdu + pctUnemploy + pctEmploy + 
pctKidsBornNevrMarr + pctHousOccup + pctHousOwnerOccup + pctVacantBoarded + pctVacant6up + 
popDensity + pctForeignBorn, data = USACrime)
vif(model1)


#LASSO Regression (violentPerPop)
USACrimeLasso.viol = subset(USACrime, select = c(violentPerPop, pctUrban, medIncome, 
pctLowEdu, pctUnemploy, pctEmploy, pctKidsBornNevrMarr, pctHousOccup, pctHousOwnerOccup, 
pctVacantBoarded, pctVacant6up, popDensity, pctForeignBorn))

exp.variables.viol &lt;- data.matrix(USACrimeLasso.viol[,2:13])
out.variables.viol &lt;- USACrimeLasso.viol$violentPerPop

#Fit and testing the lasso model
lassoviol.fit &lt;- cv.glmnet(exp.variables.viol, out.variables.viol, 
type.measure = &quot;mse&quot;, alpha = 1, family = &quot;gaussian&quot;)
lassoviol.predicted &lt;- predict(lassoviol.fit, s = lassoviol.fit$lambda.1se, 
newx = exp.variables.viol)

plot(lassoviol.fit, main = &quot;10-fold Cross-Validation for optimal Lambda&quot;) #Graph for Lambda

mean((out.variables.viol -lassoviol.predicted)^2) #Finding Mean-Squared Error

#Look at the coefficients
coef(lassoviol.fit)

#LASSO Regression (nonViolPerPop)
USACrimeLasso.nonviol = subset(USACrime, select = c(nonViolPerPop, pctUrban, 
medIncome, pctLowEdu, pctUnemploy, pctEmploy, pctKidsBornNevrMarr, pctHousOccup,  
pctHousOwnerOccup, pctVacantBoarded, pctVacant6up, popDensity, pctForeignBorn))

exp.variables.nonviol &lt;- data.matrix(USACrimeLasso.nonviol[,2:13])
out.variables.nonviol &lt;- USACrimeLasso.nonviol$nonViolPerPop

#Fit and testing the lasso model
lassononviol.fit &lt;- cv.glmnet(exp.variables.nonviol, out.variables.nonviol, 
type.measure = &quot;mse&quot;, alpha = 1, family = &quot;gaussian&quot;)
lassononviol.predicted &lt;- predict(lassononviol.fit, s = lassononviol.fit$lambda.1se, 
newx = exp.variables.nonviol)

plot(lassononviol.fit, main = &quot;10-fold Cross-Validation for optimal Lambda&quot;) #Graph for Lambda

mean((out.variables.nonviol - lassononviol.predicted)^2) #Finding Mean-Squared Error

#Look at the coefficients
coef(lassononviol.fit)

#Graph Plotting
lassoviol.predict.test &lt;- data.frame(lassoviol.predicted, out.variables.viol)
lassoviol.predict.test$State &lt;- USACrimeCopy$State #Add State and region variables back
#for analysis
lassoviol.predict.test$region &lt;- USACrimeCopy$region

lassononviol.predict.test &lt;- data.frame(lassononviol.predicted, out.variables.nonviol)
lassononviol.predict.test$State &lt;- USACrimeCopy$State #Add State and region variables 
#back for analysis
lassononviol.predict.test$region &lt;- USACrimeCopy$region

#Residual Plots
ggplot(data = lassoviol.predict.test , aes(X1,(X1-out.variables.viol))) +
  geom_point(aes(color = region)) + labs(title=&quot;Residual plot for Violent Crime&quot;)

ggplot(data = lassononviol.predict.test , aes(X1,(X1-out.variables.nonviol))) +
  geom_point(aes(color = region)) + labs(title=&quot;Residual plot for Non-Violent Crime&quot;)

#Q-Q plot analysis for both violent and non-violent model
lassoviol.predict.test$residual &lt;- (lassoviol.predict.test$X1 - 
lassoviol.predict.test$out.variables.viol)
qqnorm(lassoviol.predict.test$residual, main = &quot;Q-Q plot for violent model residuals&quot;)

lassononviol.predict.test$residual &lt;- (lassononviol.predict.test$X1 - 
lassononviol.predict.test$out.variables.nonviol)
qqnorm(lassononviol.predict.test$residual, main = &quot;Q-Q plot for non-violent model residuals&quot;)

#Facet Residual Plot
lassoviol.predict.test$viol &lt;- &quot;Violent Crimes&quot;
lassononviol.predict.test$viol &lt;- &quot;Non-Violent Crimes&quot;
viol.results &lt;- rename(lassoviol.predict.test, out.variables = out.variables.viol)
nonviol.results &lt;- rename(lassononviol.predict.test, out.variables = out.variables.nonviol)
all.results = merge(viol.results,nonviol.results,all = TRUE)
ggplot(data = all.results , aes(X1,residual)) + geom_point(aes(color = region)) + 
  labs(title=&quot;Residual plot for Non-Violent Crime and Violent Crime model&quot;,
 x = &quot;Fitted values&quot;, y = &quot;Residuals&quot;) + 
facet_wrap(~viol, scales = &quot;free&quot;) + theme(legend.position=&quot;bottom&quot;)

#Cook’s Distance for both models
lm1&lt;-lm(violentPerPop~pctUrban+medIncome+pctLowEdu+pctUnemploy+ pctKidsBornNevrMarr
       +pctHousOccup+pctHousOwnerOccup+pctVacantBoarded+pctVacant6up+pctForeignBorn)
lm2&lt;-lm(nonViolPerPop~pctUrban+medIncome+pctKidsBornNevrMarr+pctHousOccup
      +pctHousOwnerOccup+pctVacantBoarded+pctVacant6up)
par(mfrow=c(1,2))
plot(lm1,4, main=&quot;Cook’s Distance for Violent Crime&quot;)
plot(lm2,4, main=&quot;Cook’s Distance for Non-Violent Crime&quot;)

#Residual Plot with high residuals
ggplot(data = lassononviol.predict.test[which(
abs(lassononviol.predict.test$X1-lassononviol.predict.test$out.variables.nonviol) &gt; 2),] , 
aes(X1,(X1-out.variables.nonviol))) +
  geom_point(aes(color = region), size = 2.5) +
  labs(y=&quot;Residuals&quot;, x=&quot;Fitted values&quot;) +
  geom_text(aes(label=State),hjust=1, vjust=1)</code></pre>
</div>
